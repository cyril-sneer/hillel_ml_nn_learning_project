{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2288f9eb9902deb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:50:38.009084537Z",
     "start_time": "2024-02-24T16:50:37.388123077Z"
    }
   },
   "id": "b8f6cfccac9fbe14",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load models and tokenizers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f0df71a562f61cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizers = {\n",
    "    'eng': AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-uk\"),\n",
    "    'ukr': AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-uk-en\")\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:50:39.042644200Z",
     "start_time": "2024-02-24T16:50:38.008092283Z"
    }
   },
   "id": "34e6ae0af0afe659",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 17:50:39.422557: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-uk.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-uk-en.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"eng\": TFAutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-uk\"),\n",
    "    \"ukr\": TFAutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-uk-en\")\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:50:46.292913445Z",
     "start_time": "2024-02-24T16:50:39.045669684Z"
    }
   },
   "id": "c53bfb811ab721cd",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define translation routine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee4754b5e23b6b33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def translate(input_text, tokenizer, model):\n",
    "    \n",
    "    # 1. Токенайзер переводит исходный текст в вектор\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "    \n",
    "    # 2. Модель делает перевод и возвращает вектор для фразы на целевом языке\n",
    "    outputs = model.generate(input_ids)\n",
    "    \n",
    "    # 3. Токенайзер декодирует вектор во фрацу на целевом языке\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translated_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:50:46.315891421Z",
     "start_time": "2024-02-24T16:50:46.298596187Z"
    }
   },
   "id": "3d72e34ea29a5ca4",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# English -> Ukrainian"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dae3ac5270a62f74"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "translate_from = 'eng'\n",
    "\n",
    "tokenizer = tokenizers[translate_from]\n",
    "model = models[translate_from]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:50:46.338974050Z",
     "start_time": "2024-02-24T16:50:46.315933746Z"
    }
   },
   "id": "e5472851d35474ce",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_in_eng_ = \"\"\"\n",
    "Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. \n",
    "Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. \n",
    "These models support common tasks in different modalities, such as:\n",
    "\n",
    "Natural Language Processing: \n",
    "text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.\n",
    "Computer Vision: image classification, object detection, and segmentation.\n",
    "Audio: automatic speech recognition and audio classification.\n",
    "Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:50:46.340126234Z",
     "start_time": "2024-02-24T16:50:46.326177361Z"
    }
   },
   "id": "f16746413e9ac384",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_in_ukr = translate(input_text=text_in_eng_, tokenizer=tokenizer, model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:51:34.360962752Z",
     "start_time": "2024-02-24T16:50:46.332296161Z"
    }
   },
   "id": "4be305a74141b96",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Трансформатори надають вам змогу з легкістю звантажувати і тренувати стандартні моделі. За допомогою попередньо підготовлених моделей ви можете зменшити ваші обчислювальні витрати, сліди вуглецю, а також зберегти час і ресурси, необхідні для того, щоб тренувати модель з нуля. Ці моделі підтримують спільні завдання у різних модулях, зокрема: орієнтація мови: класифікація тексту, розпізнавання сутностей, відповідь на питання, коментар до мови, резюме, переклад, вибір з декількох варіантів і створення тексту. Комп' ютерне бачення: класифікація зображень, визначення об' єктів і сегментація. Звук: автоматичне розпізнавання і класифікація звуку. Групове: відповідь на питання таблиці, оптичне розпізнавання символів, отримання інформації з сканованих документів, класифікації відео та візуальних питань.\n"
     ]
    }
   ],
   "source": [
    "print(text_in_ukr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:51:34.363398363Z",
     "start_time": "2024-02-24T16:51:34.360686033Z"
    }
   },
   "id": "ae08a64d5fe79c58",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ukrainian -> English"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da63fd510fda522b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "translate_from = 'ukr'\n",
    "\n",
    "tokenizer = tokenizers[translate_from]\n",
    "model = models[translate_from]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:51:34.364299028Z",
     "start_time": "2024-02-24T16:51:34.361394204Z"
    }
   },
   "id": "76c4634ab08be0ad",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_in_eng = translate(input_text=text_in_ukr, tokenizer=tokenizer, model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:52:16.530897444Z",
     "start_time": "2024-02-24T16:51:34.361871736Z"
    }
   },
   "id": "b16574ee26ff1524",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers allow you to easily download and train standard models. With pre-trained models you can reduce your computational costs, carbon footprints, as well as save the time and resources needed to train the model from scratch. These models support common tasks in various modules, including: Language orientation: Text rating, Entity Authentication, Answer, Language Comment, Summary, Multiple Choices and Text Creation. Computer vision: Image classification, Object Definition, and Segment. Sound: Automatic audio recognition and audio classification. Group: Answer to Questions, Optical Character Authentication, get information from scanned documents, ratings, and visuals.\n"
     ]
    }
   ],
   "source": [
    "print(text_in_eng)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T16:52:16.582246220Z",
     "start_time": "2024-02-24T16:52:16.572643700Z"
    }
   },
   "id": "5e95bc18fb526e98",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33a1e1f396dbb1f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Более простым способом получения результата, является применение конвейера"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e0b1650065f6f5d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T20:08:30.489490703Z",
     "start_time": "2024-02-24T20:08:30.419319323Z"
    }
   },
   "id": "a6b1707a748923e8",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'translation_text': 'Veuillez traduire ce texte en français!'}]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(task='translation_en_to_fr')\n",
    "translator('Please, translate this text to French!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T20:16:46.370736917Z",
     "start_time": "2024-02-24T20:16:38.742692248Z"
    }
   },
   "id": "32928457630def2c",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "Также можно указать имя используемой модели и токенизатора. Более того, это рекомендуется.\n",
    "Вместе с тем, достаточно указать лишь имя модели. Токенизатор в таком случае будет выбран автоматически. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266e38833cdf105b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-uk.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'translation_text': 'Перекладіть цей текст українською'}]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(task='translation', model=\"Helsinki-NLP/opus-mt-en-uk\")\n",
    "translator('Please, translate this text to Ukrainian')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T20:19:47.838881601Z",
     "start_time": "2024-02-24T20:19:42.413021508Z"
    }
   },
   "id": "726c462530b0a370",
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
